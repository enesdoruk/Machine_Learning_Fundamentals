{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can think of an SVM classifier as fitting the\n",
    "widest possible street (represented by the parallel dashed lines) between the classes.\n",
    "This is called large margin classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The objective is to\n",
    "find a good balance between keeping the street as large as possible and limiting the\n",
    "margin violations (i.e., instances that end up in the middle of the street or even on the\n",
    "wrong side). This is called so margin classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparame‐\n",
    "ter: a smaller C value leads to a wider street but more margin violations.\n",
    "\n",
    "On the left, using a high C value the classifier makes\n",
    "fewer margin violations but ends up with a smaller margin. On the right, using a low\n",
    "C value the margin is much larger, but many instances end up on the street."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your SVM model is overfitting, you can try regularizing it by\n",
    "reducing C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "svm_clf = Pipeline((\n",
    " (\"scaler\", StandardScaler()),\n",
    " (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    " ))\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Logistic Regression classifiers, SVM classifiers do not out‐\n",
    "put probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it\n",
    "is much slower, especially with large training sets, so it is not recommended. Another\n",
    "option is to use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",\n",
    "alpha=1/(m*C)). This applies regular Stochastic Gradient Descent (see Chapter 4) to\n",
    "train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it\n",
    "can be useful to handle huge datasets that do not fit in memory (out-of-core train‐\n",
    "ing), or to handle online classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVM Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('poly_features',\n",
       "                 PolynomialFeatures(degree=3, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_svm_clf = Pipeline((\n",
    " (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    " (\"scaler\", StandardScaler()),\n",
    " (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    " ))\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at a low polynomial degree it\n",
    "cannot deal with very complex datasets, and with a high polynomial degree it creates\n",
    "a huge number of features, making the model too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, when using SVMs you can apply an almost miraculous mathematical\n",
    "technique called the kernel trick (it is explained in a moment). It makes it possible to\n",
    "get the same result as if you added many polynomial features, even with very highdegree polynomials, without actually having to add them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=5, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=1, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='poly', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    " (\"scaler\", StandardScaler()),\n",
    " (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    " ))\n",
    "poly_kernel_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " if your model is overfitting, you might want to reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\n",
    "it. The hyperparameter coef0 controls how much the model is influenced by highdegree polynomials versus low-degree polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Similarity Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique to tackle nonlinear problems is to add features computed using a\n",
    "similarity function that measures how much each instance resembles a particular\n",
    "landmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder how to select the landmarks. The simplest approach is to create a\n",
    "landmark at the location of each and every instance in the dataset. This creates many\n",
    "dimensions and thus increases the chances that the transformed training set will be\n",
    "linearly separable. The downside is that a training set with m instances and n features\n",
    "gets transformed into a training set with m instances and m features (assuming you\n",
    "drop the original features). If your training set is very large, you end up with an\n",
    "equally large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the similarity features method can be useful\n",
    "with any Machine Learning algorithm, but it may be computationally expensive to\n",
    "compute all the additional features, especially on large training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=0.001, break_ties=False, cache_size=200,\n",
       "                     class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma=5,\n",
       "                     kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline((\n",
    " (\"scaler\", StandardScaler()),\n",
    " (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    " ))\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing\n",
    "gamma makes the bell-shape curve narrower (see the left plot of Figure 5-8), and as a\n",
    "result each instance’s range of influence is smaller: the decision boundary ends up\n",
    "being more irregular, wiggling around individual instances. Conversely, a small gamma\n",
    "value makes the bell-shaped curve wider, so instances have a larger range of influ‐\n",
    "ence, and the decision boundary ends up smoother. So γ acts like a regularization\n",
    "hyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\n",
    "fitting, you should increase it (similar to the C hyperparameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " you should always try the linear\n",
    "kernel first (remember that LinearSVC is much faster than SVC(ker\n",
    "nel=\"linear\")), especially if the training set is very large or if it\n",
    "has plenty of features. If the training set is not too large, you should\n",
    "try the Gaussian RBF kernel as well; it works well in most cases.\n",
    "Then if you have spare time and computing power, you can also\n",
    "experiment with a few other kernels using cross-validation and grid\n",
    "search, especially if there are kernels specialized for your training\n",
    "set’s data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM algorithm is quite versatile: not only does it sup‐\n",
    "port linear and nonlinear classification, but it also supports linear and nonlinear\n",
    "regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "          random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVR class is the regression equivalent\n",
    "of the LinearSVC class. The LinearSVR class scales linearly with the size of the train‐\n",
    "ing set (just like the LinearSVC class), while the SVR class gets much too slow when\n",
    "the training set grows large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='scale',\n",
       "    kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
