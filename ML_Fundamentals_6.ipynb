{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like SVMs, Decision Trees are versatile Machine Learning algorithms that can per‐\n",
    "form both classification and regression tasks, and even multioutput tasks. They are\n",
    "very powerful algorithms, capable of fitting complex datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Visualizing a Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many qualities of Decision Trees is that they require\n",
    "very little data preparation. In particular, they don’t require feature\n",
    "scaling or centering at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the CART algorithm, which produces only binary\n",
    "trees: nonleaf nodes always have two children (i.e., questions only\n",
    "have yes/no answers). However, other algorithms such as ID3 can\n",
    "produce Decision Trees with nodes that have more than two chil‐\n",
    "dren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Class Probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tree_clf.predict_proba([[5, 1.5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CART Training Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the Classification And Regression Tree (CART) algorithm to train\n",
    "Decision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\n",
    "rithm first splits the training set in two subsets using a single feature k and a thres‐\n",
    "hold tk\n",
    " (e.g., “petal length ≤ 2.45 cm”). How does it choose k and tk\n",
    "? It searches for the\n",
    "pair (k, tk\n",
    ") that produces the purest subsets (weighted by their size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the CART algorithm is a greedy algorithm: it greed‐\n",
    "ily searches for an optimum split at the top level, then repeats the\n",
    "process at each level. It does not check whether or not the split will\n",
    "lead to the lowest possible impurity several levels down. A greedy\n",
    "algorithm often produces a reasonably good solution, but it is not\n",
    "guaranteed to be the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions requires traversing the Decision Tree from the root to a leaf.\n",
    "Decision Trees are generally approximately balanced, so traversing the Decision Tree\n",
    "requires going through roughly O(log2\n",
    "(m)) nodes.3\n",
    " Since each node only requires\n",
    "checking the value of one feature, the overall prediction complexity is just O(log2\n",
    "(m)),\n",
    "independent of the number of features. So predictions are very fast, even when deal‐\n",
    "ing with large training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a training complexity of O(n × m log(m)).\n",
    "For small training sets (less than a few thousand instances), Scikit-Learn can speed up\n",
    "training by presorting the data (set presort=True), but this slows down training con‐\n",
    "siderably for larger training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If left\n",
    "unconstrained, the tree structure will adapt itself to the training data, fitting it very\n",
    "closely, and most likely overfitting it. Such a model is often called a nonparametric\n",
    "model, not because it does not have any parameters (it often has a lot) but because the\n",
    "number of parameters is not determined prior to training, so the model structure is\n",
    "free to stick closely to the data. In contrast, a parametric model such as a linear model\n",
    "has a predetermined number of parameters, so its degree of freedom is limited,\n",
    "reducing the risk of overfitting (but increasing the risk of underfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\n",
    "during training. As you know by now, this is called regularization. The regularization\n",
    "hyperparameters depend on the algorithm used, but generally you can at least restrict\n",
    "the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\n",
    "max_depth hyperparameter (the default value is None, which means unlimited).\n",
    "Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_samples_split (the minimum number of samples a node must have before it can be split), min_samples_leaf (the minimum num‐\n",
    "ber of samples a leaf node must have), min_weight_fraction_leaf (same as\n",
    "min_samples_leaf but expressed as a fraction of the total number of weighted\n",
    "instances), max_leaf_nodes (maximum number of leaf nodes), and max_features\n",
    "(maximum number of features that are evaluated for splitting at each node). Increas‐\n",
    "ing min_* hyperparameters or reducing max_* hyperparameters will regularize the\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Decision Trees are prone to overfitting when dealing\n",
    "with regression tasks. Without any regularization (i.e., using the default hyperpara‐\n",
    "meters), you get the predictions on the left of Figure 6-6. It is obviously overfitting\n",
    "the training set very badly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision\n",
    "Trees love orthogonal decision boundaries (all splits are perpendicular to an axis),\n",
    "which makes them sensitive to training set rotation. \n",
    "\n",
    "More generally, the main issue with Decision Trees is that they are very sensitive to\n",
    "small variations in the training data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
