{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning and Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " if you aggregate\n",
    "the predictions of a group of predictors (such as classifiers or regressors), you will\n",
    "often get better predictions than with the best individual predictor. A group of pre‐\n",
    "dictors is called an ensemble; thus, this technique is called Ensemble Learning, and an\n",
    "Ensemble Learning algorithm is called an Ensemble method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you can train a group of Decision Tree classifiers, each on a different\n",
    "random subset of the training set. To make predictions, you just obtain the predic‐\n",
    "tions of all individual trees, then predict the class that gets the most votes (see the last\n",
    "exercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest,\n",
    "and despite its simplicity, this is one of the most powerful Machine Learning algo‐\n",
    "rithms available today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple way to create an even better classifier is to aggregate the predictions of\n",
    "each classifier and predict the class that gets the most votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the\n",
    "best classifier in the ensemble. In fact, even if each classifier is a weak learner (mean‐\n",
    "ing it does only slightly better than random guessing), the ensemble can still be a\n",
    "strong learner (achieving high accuracy), provided there are a sufficient number of\n",
    "weak learners and they are sufficiently diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods work best when the predictors are as independ‐\n",
    "ent from one another as possible. One way to get diverse classifiers\n",
    "is to train them using very different algorithms. This increases the\n",
    "chance that they will make very different types of errors, improving\n",
    "the ensemble’s accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    " estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    " voting='hard'\n",
    " )\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate class probabilities (i.e., they have a pre\n",
    "dict_proba() method), then you can tell Scikit-Learn to predict the class with the\n",
    "highest class probability, averaged over all the individual classifiers. This is called so\n",
    "voting. It often achieves higher performance than hard voting because it gives more\n",
    "weight to highly confident votes. All you need to do is replace voting=\"hard\" with\n",
    "voting=\"soft\" and ensure that all classifiers can estimate class probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Another approach is to use the same training algorithm for every\n",
    "predictor, but to train them on different random subsets of the training set. When\n",
    "sampling is performed with replacement, this method is called bagging1\n",
    " (short for\n",
    "bootstrap aggregating2\n",
    "). When sampling is performed without replacement, it is called\n",
    "pasting.\n",
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, both bagging and pasting allow training instances to be sampled sev‐\n",
    "eral times across multiple predictors, but only bagging allows training instances to be\n",
    "sampled several times for the same predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The aggregation\n",
    "function is typically the statistical mode (i.e., the most frequent prediction, just like a\n",
    "hard voting classifier) for classification, or the average for regression. Each individual\n",
    "predictor has a higher bias than if it were trained on the original training set, but\n",
    "aggregation reduces both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting in Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n_jobs param‐\n",
    "eter tells Scikit-Learn the number of CPU cores to use for training and predictions\n",
    "(–1 tells Scikit-Learn to use all available cores):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(), n_estimators=500,\n",
    " max_samples=100, bootstrap=True, n_jobs=-1\n",
    " )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaggingClassifier automatically performs soft voting\n",
    "instead of hard voting if the base classifier can estimate class proba‐\n",
    "bilities (i.e., if it has a predict_proba() method), which is the case\n",
    "with Decision Trees classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": the ensemble has a\n",
    "comparable bias but a smaller variance (it makes roughly the same number of errors\n",
    "on the training set, but the decision boundary is less irregular).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is\n",
    "trained on, so bagging ends up with a slightly higher bias than pasting, but this also\n",
    "means that predictors end up being less correlated so the ensemble’s variance is\n",
    "reduced. Overall, bagging often results in better models, which explains why it is gen‐\n",
    "erally preferred. However, if you have spare time and CPU power you can use crossvalidation to evaluate both bagging and pasting and select the one that works best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With bagging, some instances may be sampled several times for any given predictor,\n",
    "while others may not be sampled at all. By default a BaggingClassifier samples m\n",
    "training instances with replacement (bootstrap=True), where m is the size of the\n",
    "training set. This means that only about 63% of the training instances are sampled on\n",
    "average for each predictor.6\n",
    " The remaining 37% of the training instances that are not\n",
    "sampled are called out-of-bag (oob) instances. Note that they are not the same 37%\n",
    "for all predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches and Random Subspaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaggingClassifier class supports sampling the features as well. This is con‐\n",
    "trolled by two hyperparameters: max_features and bootstrap_features. They work\n",
    "the same way as max_samples and bootstrap, but for feature sampling instead of\n",
    "instance sampling. Thus, each predictor will be trained on a random subset of the\n",
    "input features.\n",
    "This is particularly useful when you are dealing with high-dimensional inputs (such\n",
    "as images). Sampling both training instances and features is called the Random\n",
    "Patches method.\n",
    "\n",
    "\n",
    " Keeping all training instances (i.e., bootstrap=False and max_sam\n",
    "ples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea\n",
    "tures smaller than 1.0) is called the Random Subspaces method Sampling features results in even more predictor diversity, trading a bit more bias for\n",
    "a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have discussed, a Random Forest9\n",
    " is an ensemble of Decision Trees, generally\n",
    "trained via the bagging method (or sometimes pasting), typically with max_samples\n",
    "set to the size of the training set. Instead of building a BaggingClassifier and pass‐\n",
    "ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier\n",
    "class, which is more convenient and optimized for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest algorithm introduces extra randomness when growing trees;\n",
    "instead of searching for the very best feature when splitting a node (see Chapter 6), it\n",
    "searches for the best feature among a random subset of features. This results in a\n",
    "greater tree diversity, which (once again) trades a higher bias for a lower variance,\n",
    "generally yielding an overall better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    " n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to make\n",
    "trees even more random by also using random thresholds for each feature rather than\n",
    "searching for the best possible thresholds (like regular Decision Trees do)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forest of such extremely random trees is simply called an Extremely Randomized\n",
    "Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a\n",
    "lower variance. It also makes Extra-Trees much faster to train than regular Random\n",
    "Forests since finding the best possible threshold for each feature at every node is one\n",
    "of the most time-consuming tasks of growing a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to tell in advance whether a RandomForestClassifier\n",
    "will perform better or worse than an ExtraTreesClassifier. Gen‐\n",
    "erally, the only way to know is to try both and compare them using\n",
    "cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " important features are likely to appear\n",
    "closer to the root of the tree, while unimportant features will often appear closer to\n",
    "the leaves (or not at all).\n",
    "\n",
    "You can\n",
    "access the result using the feature_importances_ variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method that\n",
    "can combine several weak learners into a strong learner. The general idea of most\n",
    "boosting methods is to train predictors sequentially, each trying to correct its prede‐\n",
    "cessor. There are many boosting methods available, but by far the most popular are\n",
    "AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way for a new predictor to correct its predecessor is to pay a bit more attention\n",
    "to the training instances that the predecessor underfitted. This results in new predic‐\n",
    "tors focusing more and more on the hard cases. This is the technique used by Ada‐\n",
    "Boost.\n",
    "\n",
    "As you can see, this sequential learning technique has some\n",
    "similarities with Gradient Descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\n",
    "gradually making it better.\n",
    "\n",
    " As a result, it does not scale as well as bag‐\n",
    "ging or pasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    " DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    " algorithm=\"SAMME.R\", learning_rate=0.5\n",
    " )\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your AdaBoost ensemble is overfitting the training set, you can\n",
    "try reducing the number of estimators or more strongly regulariz‐\n",
    "ing the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very popular Boosting algorithm is Gradient Boosting.\n",
    "17 Just like AdaBoost,\n",
    "Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
    "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
    "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
    "errors made by the previous predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting also works great with regression tasks). This is\n",
    "called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning_rate hyperparameter scales the contribution of each tree. If you set it\n",
    "to a low value, such as 0.1, you will need more trees in the ensemble to fit the train‐\n",
    "ing set, but the predictions will usually generalize better. This is a regularization tech‐\n",
    "nique called shrinkage. Figure 7-10 shows two GBRT ensembles trained with a low\n",
    "learning rate: the one on the left does not have enough trees to fit the training set,\n",
    "while the one on the right has too many trees and overfits the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal number of trees, you can use early stopping (see Chap‐\n",
    "ter 4). A simple way to implement this is to use the staged_predict() method: it\n",
    "returns an iterator over the predictions made by the ensemble at each stage of train‐\n",
    "ing (with one tree, two trees, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    " for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    " gbrt.n_estimators = n_estimators\n",
    " gbrt.fit(X_train, y_train)\n",
    " y_pred = gbrt.predict(X_val)\n",
    " val_error = mean_squared_error(y_val, y_pred)\n",
    " if val_error < min_val_error:\n",
    " min_val_error = val_error\n",
    " error_going_up = 0\n",
    " else:\n",
    " error_going_up += 1\n",
    " if error_going_up == 5:\n",
    " break # early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientBoostingRegressor class also supports a subsample hyperparameter,\n",
    "which specifies the fraction of training instances to be used for training each tree. For\n",
    "example, if subsample=0.25, then each tree is trained on 25% of the training instan‐\n",
    "ces, selected randomly. As you can probably guess by now, this trades a higher bias\n",
    "for a lower variance. It also speeds up training considerably. This technique is called\n",
    "Stochastic Gradient Boosting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use Gradient Boosting with other cost functions.\n",
    "This is controlled by the loss hyperparameter (see Scikit-Learn’s\n",
    "documentation for more details).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is based on a simple idea: instead of using trivial functions\n",
    "(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the first layer predictors are used to make predictions on the second (held-out)\n",
    "set (see Figure 7-14). This ensures that the predictions are “clean,” since the predictors\n",
    "never saw these instances during training. Now for each instance in the hold-out set\n",
    "there are three predicted values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
