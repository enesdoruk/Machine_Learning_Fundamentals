{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For example,\n",
    "consider the MNIST images (introduced in Chapter 3): the pixels on the image bor‐\n",
    "ders are almost always white, so you could completely drop these pixels from the\n",
    "training set without losing much information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, two neighboring pixels\n",
    "are often highly correlated: if you merge them into a single pixel (e.g., by taking the\n",
    "mean of the two pixel intensities), you will not lose much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you should first try to train\n",
    "your system with the original data before considering using dimen‐\n",
    "sionality reduction if training is too slow. In some cases, however,\n",
    "reducing the dimensionality of the training data may filter out\n",
    "some noise and unnecessary details and thus result in higher per‐\n",
    "formance (but in general it won’t; it will just speed up training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making predictions much less relia‐\n",
    "ble than in lower dimensions, since they will be based on much larger extrapolations.\n",
    "In short, the more dimensions the training set has, the greater the risk of overfitting\n",
    "it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, one solution to the curse of dimensionality could be to increase the size of\n",
    "the training set to reach a sufficient density of training instances. Unfortunately, in\n",
    "practice, the number of training instances required to reach a given density grows\n",
    "exponentially with the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Approaches for Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most real-world problems, training instances are not spread out uniformly across\n",
    "all dimensions. Many features are almost constant, while others are highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many dimensionality reduction algorithms work by modeling the manifold on which\n",
    "the training instances lie; this is called Manifold Learning. It relies on the manifold\n",
    "assumption, also called the manifold hypothesis, which holds that most real-world\n",
    "high-dimensional datasets lie close to a much lower-dimensional manifold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In other words, the degrees\n",
    "of freedom available to you if you try to create a digit image are dramatically lower\n",
    "than the degrees of freedom you would have if you were allowed to generate any\n",
    "image you wanted. These constraints tend to squeeze the dataset into a lowerdimensional manifold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, if you reduce the dimensionality of your training set before training a\n",
    "model, it will definitely speed up training, but it may not always lead to a better or\n",
    "simpler solution; it all depends on the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is by far the most popular dimensionality reduc‐\n",
    "tion algorithm. First it identifies the hyperplane that lies closest to the data, and then\n",
    "it projects the data onto it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving the Variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can project the training set onto a lower-dimensional hyperplane, you\n",
    "first need to choose the right hyperplane. \n",
    "\n",
    " the projection onto the solid line preserves the maximum\n",
    "variance, while the projection onto the dotted line preserves very little variance, and\n",
    "the projection onto the dashed line preserves an intermediate amount of variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems reasonable to select the axis that preserves the maximum amount of var‐\n",
    "iance, as it will most likely lose less information than the other projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA identifies the axis that accounts for the largest amount of variance in the train‐\n",
    "ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the\n",
    "first one, that accounts for the largest amount of remaining variance. \n",
    "\n",
    "The unit vector that defines the ith axis is called the ith principal component (PC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA assumes that the dataset is centered around the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting Down to d Dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Selecting this hyperplane ensures that the\n",
    "projection will preserve as much variance as possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very useful piece of information is the explained variance ratio of each prin‐\n",
    "cipal component, available via the explained_variance_ratio_ variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\n",
    "lies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\n",
    "able to assume that it probably carries little information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless, of course, you are reducing dimen‐\n",
    "sionality for data visualization—in that case you will generally want to reduce the\n",
    "dimensionality down to 2 or 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could then set n_components=d and run PCA again. However, there is a much\n",
    "better option: instead of specifying the number of principal components you want to\n",
    "preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the\n",
    "ratio of variance you wish to preserve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another option is to plot the explained variance as a function of the number of\n",
    "dimensions (simply plot cumsum; see Figure 8-8). There will usually be an elbow in the\n",
    "curve, where the explained variance stops growing fast. You can think of this as the\n",
    "intrinsic dimensionality of the dataset. In this case, you can see that reducing the\n",
    "dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\n",
    "iance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously after dimensionality reduction, the training set takes up much less space.\n",
    "For example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\n",
    "iance. You should find that each instance will have just over 150 features, instead of\n",
    "the original 784 features. So while most of the variance is preserved, the dataset is\n",
    "now less than 20% of its original size! This is a reasonable compression ratio, and you\n",
    "can see how this can speed up a classification algorithm (such as an SVM classifier)\n",
    "tremendously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist)\n",
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with the preceding implementation of PCA is that it requires the whole\n",
    "training set to fit in memory in order for the SVD algorithm to run. Fortunately,\n",
    "Incremental PCA (IPCA) algorithms have been developed: you can split the training\n",
    "set into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\n",
    "useful for large training sets, and also to apply PCA online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    " inc_pca.partial_fit(X_batch)\n",
    "X_mnist_reduced = inc_pca.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This\n",
    "is a stochastic algorithm that quickly finds an approximation of the first d principal\n",
    "components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the same trick can be applied to PCA, making it possible to perform\n",
    "complex nonlinear projections for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kPCA is an unsupervised learning algorithm, there is no obvious performance\n",
    "measure to help you select the best kernel and hyperparameter values. \n",
    "\n",
    " For example, the following\n",
    "code creates a two-step pipeline, first reducing dimensionality to two dimensions\n",
    "using kPCA, then applying Logistic Regression for classification. Then it uses Grid\n",
    "SearchCV to find the best kernel and gamma value for kPCA in order to get the best\n",
    "classification accuracy at the end of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline([\n",
    " (\"kpca\", KernelPCA(n_components=2)),\n",
    " (\"log_reg\", LogisticRegression())\n",
    " ])\n",
    "param_grid = [{\n",
    " \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    " \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    " }]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fortunately, it is pos‐\n",
    "sible to find a point in the original space that would map close to the reconstructed\n",
    "point. This is called the reconstruction pre-image. Once you have this pre-image, you\n",
    "can measure its squared distance to the original instance. You can then select the ker‐\n",
    "nel and hyperparameters that minimize this reconstruction pre-image error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
    " fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally Linear Embedding (LLE)8\n",
    " is another very powerful nonlinear dimensionality\n",
    "reduction (NLDR) technique. It is a Manifold Learning technique that does not rely\n",
    "on projections like the previous algorithms. In a nutshell, LLE works by first measur‐\n",
    "ing how each training instance linearly relates to its closest neighbors (c.n.), and then\n",
    "looking for a low-dimensional representation of the training set where these local\n",
    "relationships are best preserved (more details shortly). This makes it particularly\n",
    "good at unrolling twisted manifolds, especially when there is not too much noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting 2D dataset is shown in Figure 8-12. As you can\n",
    "see, the Swiss roll is completely unrolled and the distances between instances are\n",
    "locally well preserved. However, distances are not preserved on a larger scale: the left\n",
    "part of the unrolled Swiss roll is squeezed, while the right part is stretched. Neverthe‐\n",
    "less, LLE did a pretty good job at modeling the manifold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dimensionality Reduction Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve\n",
    "the distances between the instances (see Figure 8-13).\n",
    "\n",
    "• Isomap creates a graph by connecting each instance to its nearest neighbors, then\n",
    "reduces dimensionality while trying to preserve the geodesic distances9 between\n",
    "the instances.\n",
    "\n",
    "\n",
    "• t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality\n",
    "while trying to keep similar instances close and dissimilar instances apart. It is\n",
    "mostly used for visualization, in particular to visualize clusters of instances in\n",
    "high-dimensional space (e.g., to visualize the MNIST images in 2D).\n",
    "\n",
    "• Linear Discriminant Analysis (LDA) is actually a classification algorithm, but dur‐\n",
    "ing training it learns the most discriminative axes between the classes, and these\n",
    "axes can then be used to define a hyperplane onto which to project the data. The\n",
    "benefit is that the projection will keep classes as far apart as possible, so LDA is a\n",
    "good technique to reduce dimensionality before running another classification\n",
    "algorithm such as an SVM classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
